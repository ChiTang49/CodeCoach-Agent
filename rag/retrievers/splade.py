"""
SPLADE Learned Sparse Retriever
ä½¿ç”¨é¢„è®­ç»ƒ SPLADE æ¨¡å‹å°†æ–‡æ¡£å’ŒæŸ¥è¯¢è½¬ä¸ºç¨€ç–å‘é‡ï¼Œé€šè¿‡ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢ã€‚
æ¨¡å‹ï¼šnaver/splade-cocondenser-ensembledistil
"""
import os
import json
import logging
from typing import List, Dict, Optional
from pathlib import Path

import torch
from transformers import AutoModelForMaskedLM, AutoTokenizer

from rag.models import KnowledgeChunk, RetrievedChunk

logger = logging.getLogger(__name__)

# é»˜è®¤æ¨¡å‹
DEFAULT_SPLADE_MODEL = "naver/splade-cocondenser-ensembledistil"

# å¦‚æœ HF_HOME æœªè®¾ï¼Œé»˜è®¤æŒ‡å‘é¡¹ç›®æ‰€åœ¨ç›˜ç¬¦é¿å… C ç›˜ç©ºé—´ä¸è¶³
if not os.environ.get("HF_HOME"):
    _drive = os.path.splitdrive(os.path.abspath(__file__))[0]  # e.g. "F:"
    os.environ["HF_HOME"] = os.path.join(_drive + os.sep, "hf_cache")

# ç´¢å¼•æŒä¹…åŒ–ç›®å½•ï¼ˆä¸ BM25 å…±ç”¨ rag_data/ï¼‰
SPLADE_INDEX_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "rag_data")
SPLADE_INDEX_FILE = "splade_index.json"
SPLADE_CHUNKS_FILE = "splade_chunks.json"


class SpladeRetriever:
    """
    SPLADE Learned Sparse Retriever

    é€šè¿‡ SPLADE æ¨¡å‹å°†æ–‡æœ¬è½¬ä¸ºè¯æ±‡è¡¨ç»´åº¦ä¸Šçš„ç¨€ç–æƒé‡å‘é‡ï¼Œ
    åˆ©ç”¨ç¨€ç–å‘é‡ç‚¹ç§¯è¿›è¡Œé«˜æ•ˆæ£€ç´¢ã€‚
    """

    def __init__(self, index_path: Optional[str] = None, model_name: str = DEFAULT_SPLADE_MODEL):
        """
        åˆå§‹åŒ– SPLADE æ£€ç´¢å™¨ã€‚

        Args:
            index_path: é¢„æ„å»ºç´¢å¼•æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œä¸º None æ—¶ä½¿ç”¨é»˜è®¤è·¯å¾„
            model_name: HuggingFace æ¨¡å‹åç§°
        """
        self.index_dir = index_path or SPLADE_INDEX_DIR
        self.model_name = model_name

        # æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._tokenizer = None
        self._model = None
        self._device = None

        # ç´¢å¼•æ•°æ®
        self.sparse_index: Dict[str, Dict[str, float]] = {}  # chunk_id -> {token_id: weight}
        self.chunks: List[KnowledgeChunk] = []
        self.chunk_id_to_idx: Dict[str, int] = {}
        self._inverted_index: Dict[str, Dict[str, float]] = {}  # token_id -> {chunk_id: weight}

        # å°è¯•ä»ç£ç›˜åŠ è½½å·²æœ‰ç´¢å¼•
        self._try_load_index()

    def _ensure_model(self):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½ï¼ˆå»¶è¿ŸåŠ è½½ä»¥é¿å…æœªä½¿ç”¨æ—¶å ç”¨æ˜¾å­˜ï¼‰"""
        if self._model is not None:
            return

        logger.info(f"åŠ è½½ SPLADE æ¨¡å‹: {self.model_name}")
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self._model = AutoModelForMaskedLM.from_pretrained(
            self.model_name, use_safetensors=True
        ).to(self._device)
        self._model.eval()
        logger.info(f"SPLADE æ¨¡å‹åŠ è½½å®Œæˆ (device={self._device})")

    def _encode_sparse(self, text: str) -> Dict[str, float]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸º SPLADE ç¨€ç–å‘é‡ã€‚

        SPLADE è¾“å‡ºæ¯ä¸ªè¯æ±‡ token çš„é‡è¦æ€§æƒé‡ï¼ˆç»è¿‡ log(1+ReLU) å˜æ¢ï¼‰ï¼Œ
        åªä¿ç•™æƒé‡ > 0 çš„ tokenã€‚

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            ç¨€ç–å‘é‡ {token_id_str: weight}
        """
        self._ensure_model()

        tokens = self._tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True,
        ).to(self._device)

        with torch.no_grad():
            output = self._model(**tokens)

        # SPLADE èšåˆï¼šå¯¹åºåˆ—ç»´åº¦å– maxï¼Œå†åš log(1 + ReLU(x))
        logits = output.logits  # (1, seq_len, vocab_size)
        # ä½¿ç”¨ attention_mask é®è”½ padding token
        attention_mask = tokens["attention_mask"].unsqueeze(-1)  # (1, seq_len, 1)
        logits = logits * attention_mask

        sparse_vec = torch.max(
            torch.log1p(torch.relu(logits)),
            dim=1
        ).values.squeeze(0)  # (vocab_size,)

        # æå–éé›¶é¡¹
        non_zero = sparse_vec.nonzero(as_tuple=True)[0]
        sparse_dict = {}
        for idx in non_zero:
            token_id = str(idx.item())
            weight = sparse_vec[idx].item()
            if weight > 0:
                sparse_dict[token_id] = round(weight, 6)

        return sparse_dict

    def build_index(self, chunks: List[KnowledgeChunk], batch_size: int = 16):
        """
        æ„å»º SPLADE ç¨€ç–ç´¢å¼•ã€‚

        å°†æ‰€æœ‰ chunk è½¬ä¸ºç¨€ç–å‘é‡å¹¶ä¿å­˜ä¸ºæœ¬åœ°ç´¢å¼•æ–‡ä»¶ã€‚

        Args:
            chunks: KnowledgeChunk åˆ—è¡¨
            batch_size: æ‰¹é‡ç¼–ç å¤§å°ï¼ˆæ§åˆ¶æ˜¾å­˜ï¼‰
        """
        self._ensure_model()

        print(f"ğŸ”„ æ„å»º SPLADE ç´¢å¼•ï¼ˆ{len(chunks)} ä¸ª chunkï¼‰...")
        self.chunks = chunks
        self.sparse_index = {}
        self.chunk_id_to_idx = {}

        for i, chunk in enumerate(chunks):
            self.chunk_id_to_idx[chunk.chunk_id] = i
            sparse_vec = self._encode_sparse(chunk.content)
            self.sparse_index[chunk.chunk_id] = sparse_vec

            if (i + 1) % 50 == 0 or (i + 1) == len(chunks):
                print(f"   å·²ç¼–ç : {i + 1}/{len(chunks)}")

        print(f"âœ… SPLADE ç´¢å¼•æ„å»ºå®Œæˆ")

        # æ„å»ºå€’æ’ç´¢å¼•
        self._build_inverted_index()

        # æŒä¹…åŒ–
        self._save_index()

    def _build_inverted_index(self):
        """ä» sparse_index æ„å»ºå€’æ’ç´¢å¼•ï¼Œç”¨äºåŠ é€Ÿæ£€ç´¢"""
        inv: Dict[str, Dict[str, float]] = {}
        for chunk_id, token_weights in self.sparse_index.items():
            for token_id, weight in token_weights.items():
                if token_id not in inv:
                    inv[token_id] = {}
                inv[token_id][chunk_id] = weight
        self._inverted_index = inv

    def retrieve(self, query: str, top_k: int = 10) -> List[RetrievedChunk]:
        """
        ä½¿ç”¨ SPLADE è¿›è¡Œ learned sparse æ£€ç´¢ã€‚

        å°† query è½¬ä¸ºç¨€ç–å‘é‡ï¼Œä¸ç´¢å¼•ä¸­æ¯ä¸ª chunk çš„ç¨€ç–å‘é‡
        è®¡ç®—ç‚¹ç§¯ï¼ˆé‡å  token æƒé‡ä¹˜ç§¯ä¹‹å’Œï¼‰ï¼Œè¿”å› top_kã€‚

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›å‰ k ä¸ªç»“æœ

        Returns:
            RetrievedChunk åˆ—è¡¨
        """
        if not self.sparse_index:
            logger.warning("SPLADE ç´¢å¼•æœªåˆå§‹åŒ–")
            return []

        query_sparse = self._encode_sparse(query)

        # ä½¿ç”¨å€’æ’ç´¢å¼•åŠ é€Ÿç‚¹ç§¯è®¡ç®—ï¼ˆåªè®¿é—® query token å‘½ä¸­çš„æ–‡æ¡£ï¼‰
        score_map: Dict[str, float] = {}
        if self._inverted_index:
            for token_id, q_weight in query_sparse.items():
                posting = self._inverted_index.get(token_id)
                if posting:
                    for chunk_id, d_weight in posting.items():
                        score_map[chunk_id] = score_map.get(chunk_id, 0.0) + q_weight * d_weight
        else:
            # å›é€€åˆ°åŸå§‹éå†æ–¹å¼
            for chunk_id, doc_sparse in self.sparse_index.items():
                score = self._dot_product(query_sparse, doc_sparse)
                if score > 0:
                    score_map[chunk_id] = score

        # æ’åºå– top_k
        sorted_scores = sorted(score_map.items(), key=lambda x: x[1], reverse=True)[:top_k]

        retrieved = []
        for chunk_id, score in sorted_scores:
            idx = self.chunk_id_to_idx.get(chunk_id)
            if idx is not None and idx < len(self.chunks):
                retrieved.append(RetrievedChunk(
                    chunk=self.chunks[idx],
                    retriever_type="splade",
                    score=score,
                ))

        return retrieved

    @staticmethod
    def _dot_product(vec_a: Dict[str, float], vec_b: Dict[str, float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªç¨€ç–å‘é‡çš„ç‚¹ç§¯ã€‚

        score = sum(a_i * b_i) for overlapping tokens
        """
        # ä»¥è¾ƒå°çš„å‘é‡ä¸ºä¸»å¾ªç¯
        if len(vec_a) > len(vec_b):
            vec_a, vec_b = vec_b, vec_a

        score = 0.0
        for token_id, weight_a in vec_a.items():
            weight_b = vec_b.get(token_id, 0.0)
            score += weight_a * weight_b
        return score

    def _save_index(self):
        """æŒä¹…åŒ– SPLADE ç´¢å¼•å’Œ chunk æ•°æ®åˆ°ç£ç›˜"""
        os.makedirs(self.index_dir, exist_ok=True)

        # ä¿å­˜ç¨€ç–ç´¢å¼•
        index_path = os.path.join(self.index_dir, SPLADE_INDEX_FILE)
        with open(index_path, "w", encoding="utf-8") as f:
            json.dump(self.sparse_index, f, ensure_ascii=False)
        logger.info(f"SPLADE ç´¢å¼•å·²ä¿å­˜åˆ° {index_path}")

        # ä¿å­˜ chunk å…ƒæ•°æ®
        chunks_path = os.path.join(self.index_dir, SPLADE_CHUNKS_FILE)
        data = []
        for chunk in self.chunks:
            data.append({
                "chunk_id": chunk.chunk_id,
                "content": chunk.content,
                "source": chunk.source,
                "chapter": chunk.chapter,
                "section": chunk.section,
                "keywords": chunk.keywords,
            })
        with open(chunks_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"SPLADE chunk æ•°æ®å·²ä¿å­˜åˆ° {chunks_path}")

    def _try_load_index(self):
        """ä»ç£ç›˜åŠ è½½å·²æœ‰çš„ SPLADE ç´¢å¼•"""
        index_path = os.path.join(self.index_dir, SPLADE_INDEX_FILE)
        chunks_path = os.path.join(self.index_dir, SPLADE_CHUNKS_FILE)

        if not os.path.exists(index_path) or not os.path.exists(chunks_path):
            logger.info("æœªæ‰¾åˆ° SPLADE ç´¢å¼•æ–‡ä»¶ï¼Œç­‰å¾…æ„å»º")
            return

        try:
            # åŠ è½½ç¨€ç–ç´¢å¼•
            with open(index_path, "r", encoding="utf-8") as f:
                self.sparse_index = json.load(f)

            # åŠ è½½ chunk æ•°æ®
            with open(chunks_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            self.chunks = []
            self.chunk_id_to_idx = {}
            for i, item in enumerate(data):
                chunk = KnowledgeChunk(
                    chunk_id=item["chunk_id"],
                    content=item["content"],
                    source=item["source"],
                    chapter=item.get("chapter", ""),
                    section=item.get("section", ""),
                    keywords=item.get("keywords", []),
                )
                self.chunks.append(chunk)
                self.chunk_id_to_idx[chunk.chunk_id] = i

            print(f"âœ… ä»ç£ç›˜åŠ è½½ SPLADE ç´¢å¼•ï¼ˆ{len(self.chunks)} ä¸ª chunkï¼Œ{len(self.sparse_index)} æ¡ç¨€ç–å‘é‡ï¼‰")
            # æ„å»ºå€’æ’ç´¢å¼•åŠ é€Ÿæ£€ç´¢
            self._build_inverted_index()
        except Exception as e:
            logger.error(f"åŠ è½½ SPLADE ç´¢å¼•å¤±è´¥: {e}")
            self.sparse_index = {}
            self.chunks = []
            self.chunk_id_to_idx = {}
