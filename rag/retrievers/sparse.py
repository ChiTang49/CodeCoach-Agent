"""
Sparse Retriever - åŸºäº BM25 çš„å…³é”®è¯æ£€ç´¢å™¨
é€‚åˆç²¾ç¡®æœ¯è¯­åŒ¹é…å’Œè‹±æ–‡ç¼©å†™æ£€ç´¢
"""
import json
import os
import logging
from typing import List, Optional
from pathlib import Path

import jieba
from rank_bm25 import BM25Okapi

from rag.models import KnowledgeChunk, RetrievedChunk

logger = logging.getLogger(__name__)

# BM25 ç´¢å¼•æŒä¹…åŒ–è·¯å¾„
BM25_INDEX_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "rag_data")


class SparseRetriever:
    """BM25 å…³é”®è¯æ£€ç´¢å™¨"""
    
    def __init__(self, chunks: Optional[List[KnowledgeChunk]] = None):
        """
        Args:
            chunks: å·²è§£æçš„ chunk åˆ—è¡¨ï¼ˆç”¨äºæ„å»ºç´¢å¼•ï¼‰ï¼›
                    å¦‚æœä¸º Noneï¼Œåˆ™å°è¯•ä»ç£ç›˜åŠ è½½ç´¢å¼•
        """
        self.chunks: List[KnowledgeChunk] = []
        self.bm25: Optional[BM25Okapi] = None
        self.tokenized_corpus: List[List[str]] = []
        
        if chunks:
            self.build_index(chunks)
        else:
            self._try_load_index()
    
    def build_index(self, chunks: List[KnowledgeChunk]):
        """
        æ„å»º BM25 ç´¢å¼•
        """
        print(f"ğŸ”„ æ„å»º BM25 ç´¢å¼•ï¼ˆ{len(chunks)} ä¸ª chunkï¼‰...")
        self.chunks = chunks
        
        # jieba åˆ†è¯
        self.tokenized_corpus = []
        for chunk in chunks:
            tokens = list(jieba.cut(chunk.content))
            # åŒæ—¶åŠ å…¥ keywords æå‡æœ¯è¯­æƒé‡
            tokens.extend(chunk.keywords)
            self.tokenized_corpus.append(tokens)
        
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        print(f"âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆ")
        
        # æŒä¹…åŒ–
        self._save_index()
    
    def retrieve(self, query: str, top_k: int = 10) -> List[RetrievedChunk]:
        """
        ä½¿ç”¨ BM25 è¿›è¡Œå…³é”®è¯æ£€ç´¢
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            
        Returns:
            RetrievedChunk åˆ—è¡¨
        """
        if self.bm25 is None:
            logger.warning("BM25 ç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        query_tokens = list(jieba.cut(query))
        scores = self.bm25.get_scores(query_tokens)
        
        # è·å– top_k ç´¢å¼•
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        
        retrieved = []
        for idx in top_indices:
            if scores[idx] > 0:
                retrieved.append(RetrievedChunk(
                    chunk=self.chunks[idx],
                    retriever_type="bm25",
                    score=float(scores[idx])
                ))
        
        return retrieved
    
    def _save_index(self):
        """æŒä¹…åŒ– chunk æ•°æ®åˆ°ç£ç›˜ï¼ˆBM25 ç´¢å¼•æ¯æ¬¡ä» chunk é‡å»ºï¼‰"""
        os.makedirs(BM25_INDEX_DIR, exist_ok=True)
        index_path = os.path.join(BM25_INDEX_DIR, "bm25_chunks.json")
        
        data = []
        for chunk in self.chunks:
            data.append({
                "chunk_id": chunk.chunk_id,
                "content": chunk.content,
                "source": chunk.source,
                "chapter": chunk.chapter,
                "section": chunk.section,
                "keywords": chunk.keywords,
            })
        
        with open(index_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"BM25 chunk æ•°æ®å·²ä¿å­˜åˆ° {index_path}")
    
    def _try_load_index(self):
        """ä»ç£ç›˜åŠ è½½ chunk æ•°æ®å¹¶é‡å»º BM25 ç´¢å¼•"""
        index_path = os.path.join(BM25_INDEX_DIR, "bm25_chunks.json")
        if not os.path.exists(index_path):
            logger.info("æœªæ‰¾åˆ° BM25 ç´¢å¼•æ–‡ä»¶ï¼Œç­‰å¾…æ„å»º")
            return
        
        try:
            with open(index_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chunks = []
            for item in data:
                chunks.append(KnowledgeChunk(
                    chunk_id=item["chunk_id"],
                    content=item["content"],
                    source=item["source"],
                    chapter=item.get("chapter", ""),
                    section=item.get("section", ""),
                    keywords=item.get("keywords", []),
                ))
            
            if chunks:
                self.build_index(chunks)
                print(f"âœ… ä»ç£ç›˜åŠ è½½ BM25 ç´¢å¼•ï¼ˆ{len(chunks)} ä¸ª chunkï¼‰")
        except Exception as e:
            logger.error(f"åŠ è½½ BM25 ç´¢å¼•å¤±è´¥: {e}")
