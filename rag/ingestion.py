"""
PDF æ–‡æ¡£é¢„å¤„ç†æ¨¡å—
å°† PDF æ–‡ä»¶è§£æä¸ºç»“æ„åŒ–çš„ KnowledgeChunk åˆ—è¡¨
æ”¯æŒç« èŠ‚/å°èŠ‚åˆ‡åˆ†ã€chunk åˆ†å‰²å’Œ metadata ç”Ÿæˆ
"""
import os
import re
import hashlib
from typing import List, Tuple, Optional
from pathlib import Path

import fitz  # PyMuPDF
import jieba

from rag.models import KnowledgeChunk


# ========================
# é…ç½®å¸¸é‡
# ========================
CHUNK_MAX_CHARS = 800       # æ¯ä¸ª chunk æœ€å¤§å­—ç¬¦æ•° (~300-500 tokens ä¸­æ–‡)
CHUNK_OVERLAP_CHARS = 100   # chunk ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
MIN_CHUNK_CHARS = 50        # æœ€å° chunk å­—ç¬¦æ•°ï¼ˆè¿‡çŸ­çš„ä¸¢å¼ƒï¼‰


def _generate_chunk_id(source: str, chapter: str, section: str, idx: int) -> str:
    """ç”Ÿæˆå”¯ä¸€çš„ chunk ID"""
    raw = f"{source}::{chapter}::{section}::{idx}"
    return hashlib.md5(raw.encode("utf-8")).hexdigest()[:16]


def _extract_keywords(text: str, top_k: int = 8) -> List[str]:
    """
    ä½¿ç”¨ jieba æå–å…³é”®è¯
    """
    import jieba.analyse
    keywords = jieba.analyse.extract_tags(text, topK=top_k)
    return keywords


def _is_chapter_heading(line: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜ï¼ˆä¸€çº§æ ‡é¢˜ï¼‰"""
    line = line.strip()
    # åŒ¹é…ä¸­æ–‡æ•°å­—ç« èŠ‚: ç¬¬ä¸€ç« ã€ç¬¬äºŒç«  ç­‰
    if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+[ç« ç¯‡]', line):
        return True
    # åŒ¹é…é˜¿æ‹‰ä¼¯æ•°å­—ä¸€çº§æ ‡é¢˜: 1. xxx, 1 xxx
    if re.match(r'^\d{1,2}[\.\s]\s*\S', line) and len(line) < 60:
        return True
    # åŒ¹é…å…¨å¤§å†™è‹±æ–‡æˆ–é‡è¦å…³é”®è¯å¼€å¤´ï¼ˆOI-wiki é£æ ¼ï¼‰
    if re.match(r'^[A-Z][A-Za-z\s\-]+$', line) and len(line) < 80:
        return True
    # åŒ¹é…ç‰¹å®šå¤§æ ‡é¢˜æ¨¡å¼
    if re.match(r'^(åŸºç¡€|æœç´¢|åŠ¨æ€è§„åˆ’|å­—ç¬¦ä¸²|æ•°å­¦|æ•°æ®ç»“æ„|å›¾è®º|æ‚é¡¹|å‡ ä½•|è¯­è¨€åŸºç¡€|ç«èµ›)', line) and len(line) < 40:
        return True
    return False


def _is_section_heading(line: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå°èŠ‚æ ‡é¢˜ï¼ˆäºŒçº§åŠä»¥ä¸‹æ ‡é¢˜ï¼‰"""
    line = line.strip()
    # åŒ¹é… 1.1 xxx, 1.2.3 xxx ç­‰å¤šçº§ç¼–å·
    if re.match(r'^\d{1,2}\.\d{1,2}', line) and len(line) < 80:
        return True
    # åŒ¹é…ä¸­æ–‡"ç¬¬XèŠ‚"
    if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[èŠ‚]', line):
        return True
    # åŒ¹é…ç®€çŸ­æ ‡é¢˜è¡Œï¼ˆé€šå¸¸æ˜¯åŠ ç²—æ ‡é¢˜è¢«æå–åçš„çº¯æ–‡æœ¬ï¼‰
    if len(line) < 50 and not line.endswith('ã€‚') and not line.endswith('ï¼›') and len(line) > 2:
        # å«æœ‰ä¸­æ–‡ä¸”ä¸å«æ ‡ç‚¹è¿‡å¤š
        if re.search(r'[\u4e00-\u9fff]', line) and line.count('ï¼Œ') == 0 and line.count('ã€‚') == 0:
            # å†æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯å°èŠ‚æ ‡é¢˜ï¼ˆä¸ä»¥æ•°å­—æˆ–ç¬¦å·ç»“å°¾ï¼‰
            if not re.search(r'[\d\.\,\;\:\!]$', line):
                return False  # ä¿å®ˆç­–ç•¥ï¼Œä¸è½»æ˜“åˆ¤å®š
    return False


def _split_text_into_chunks(
    text: str,
    max_chars: int = CHUNK_MAX_CHARS,
    overlap: int = CHUNK_OVERLAP_CHARS
) -> List[str]:
    """
    å°†é•¿æ–‡æœ¬åˆ‡åˆ†æˆå›ºå®šå¤§å°çš„ chunkï¼Œæ”¯æŒé‡å 
    ä¼˜å…ˆåœ¨æ®µè½è¾¹ç•Œå¤„åˆ‡åˆ†
    """
    if len(text) <= max_chars:
        return [text] if len(text) >= MIN_CHUNK_CHARS else []

    chunks = []
    # å…ˆæŒ‰æ®µè½åˆ†
    paragraphs = re.split(r'\n{2,}', text)
    
    current_chunk = ""
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # å¦‚æœå½“å‰æ®µè½æœ¬èº«å°±å¾ˆé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
        if len(para) > max_chars:
            # å…ˆæŠŠå½“å‰ chunk ä¿å­˜
            if current_chunk and len(current_chunk) >= MIN_CHUNK_CHARS:
                chunks.append(current_chunk.strip())
                current_chunk = ""
            
            # æŒ‰å¥å­åˆ‡åˆ†é•¿æ®µè½
            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\.\!\?])', para)
            for sent in sentences:
                sent = sent.strip()
                if not sent:
                    continue
                if len(current_chunk) + len(sent) <= max_chars:
                    current_chunk += sent
                else:
                    if current_chunk and len(current_chunk) >= MIN_CHUNK_CHARS:
                        chunks.append(current_chunk.strip())
                    # ä¿ç•™é‡å 
                    if overlap > 0 and current_chunk:
                        current_chunk = current_chunk[-overlap:] + sent
                    else:
                        current_chunk = sent
        else:
            if len(current_chunk) + len(para) + 1 <= max_chars:
                current_chunk += "\n" + para if current_chunk else para
            else:
                if current_chunk and len(current_chunk) >= MIN_CHUNK_CHARS:
                    chunks.append(current_chunk.strip())
                # ä¿ç•™é‡å 
                if overlap > 0 and current_chunk:
                    current_chunk = current_chunk[-overlap:] + "\n" + para
                else:
                    current_chunk = para

    if current_chunk and len(current_chunk) >= MIN_CHUNK_CHARS:
        chunks.append(current_chunk.strip())

    return chunks


def parse_pdf(pdf_path: str, source_name: Optional[str] = None) -> List[KnowledgeChunk]:
    """
    è§£æ PDF æ–‡ä»¶ä¸º KnowledgeChunk åˆ—è¡¨
    
    Args:
        pdf_path: PDF æ–‡ä»¶è·¯å¾„
        source_name: æ¥æºåç§°ï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
    
    Returns:
        KnowledgeChunk åˆ—è¡¨
    """
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
    
    source = source_name or pdf_path.name
    print(f"ğŸ“„ å¼€å§‹è§£æ PDF: {source}")
    
    doc = fitz.open(str(pdf_path))
    total_pages = len(doc)
    print(f"   æ€»é¡µæ•°: {total_pages}")
    
    # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
    all_text = []
    for page_num in range(total_pages):
        page = doc[page_num]
        text = page.get_text("text")
        if text.strip():
            all_text.append(text)
    
    full_text = "\n".join(all_text)
    doc.close()
    print(f"   æå–æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
    
    # ç¬¬äºŒæ­¥ï¼šæŒ‰ç« èŠ‚åˆ†å‰²æ–‡æœ¬
    sections = _split_into_sections(full_text)
    print(f"   è¯†åˆ«åˆ° {len(sections)} ä¸ªç« èŠ‚/å°èŠ‚")
    
    # ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯ä¸ª section è¿›è¡Œ chunk åˆ‡åˆ†
    chunks = []
    chunk_idx = 0
    for chapter, section, section_text in sections:
        text_chunks = _split_text_into_chunks(section_text)
        for text in text_chunks:
            keywords = _extract_keywords(text)
            chunk = KnowledgeChunk(
                chunk_id=_generate_chunk_id(source, chapter, section, chunk_idx),
                content=text,
                source=source,
                chapter=chapter,
                section=section,
                keywords=keywords
            )
            chunks.append(chunk)
            chunk_idx += 1
    
    print(f"âœ… è§£æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ª chunk")
    return chunks


def _split_into_sections(text: str) -> List[Tuple[str, str, str]]:
    """
    å°†å…¨æ–‡æŒ‰ç« èŠ‚/å°èŠ‚æ ‡é¢˜åˆ†å‰²
    
    Returns:
        [(chapter, section, text), ...]
    """
    lines = text.split('\n')
    sections = []
    
    current_chapter = "æœªåˆ†ç±»"
    current_section = ""
    current_text_lines = []
    
    for line in lines:
        stripped = line.strip()
        if not stripped:
            current_text_lines.append("")
            continue
        
        if _is_chapter_heading(stripped):
            # ä¿å­˜ä¹‹å‰çš„ section
            if current_text_lines:
                section_text = "\n".join(current_text_lines).strip()
                if section_text and len(section_text) >= MIN_CHUNK_CHARS:
                    sections.append((current_chapter, current_section, section_text))
            current_chapter = stripped[:60]  # æˆªå–æ ‡é¢˜
            current_section = ""
            current_text_lines = []
        elif _is_section_heading(stripped):
            # ä¿å­˜ä¹‹å‰çš„ section
            if current_text_lines:
                section_text = "\n".join(current_text_lines).strip()
                if section_text and len(section_text) >= MIN_CHUNK_CHARS:
                    sections.append((current_chapter, current_section, section_text))
            current_section = stripped[:60]
            current_text_lines = []
        else:
            current_text_lines.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ª section
    if current_text_lines:
        section_text = "\n".join(current_text_lines).strip()
        if section_text and len(section_text) >= MIN_CHUNK_CHARS:
            sections.append((current_chapter, current_section, section_text))
    
    # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ä»»ä½•ç« èŠ‚ç»“æ„ï¼Œå°†å…¨æ–‡ä½œä¸ºä¸€ä¸ª section
    if not sections:
        sections = [("æœªåˆ†ç±»", "", text)]
    
    return sections
