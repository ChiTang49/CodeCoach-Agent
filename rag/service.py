"""
RAGService - RAG ç»Ÿä¸€æœåŠ¡æ¥å£
# Agent å’Œ Frontend åªéœ€è°ƒç”¨ RAGService.answer() å³å¯å®Œæˆå®Œæ•´ RAG æµç¨‹
"""
import os
import time
import logging
from typing import List, Optional
from dotenv import load_dotenv
from openai import OpenAI

from qdrant_client import QdrantClient

from rag.models import KnowledgeChunk, RetrievedChunk, RAGResult
from rag.embedding import EmbeddingClient
from rag.retrievers.dense import DenseRetriever
from rag.retrievers.sparse import SparseRetriever
from rag.retrievers.section import SectionRetriever
from rag.retrievers.multi import MultiRetriever
from rag.retrievers.splade import SpladeRetriever
from rag.reranker import LLMReranker
from rag.rewrite.query_rewrite import rewrite_query
from rag.fusion.rrf import reciprocal_rank_fusion

load_dotenv()
logger = logging.getLogger(__name__)

ANSWER_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®—æ³•çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒçŸ¥è¯†ç‰‡æ®µå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åªæ ¹æ®æä¾›çš„å‚è€ƒçŸ¥è¯†å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. å¦‚æœå‚è€ƒçŸ¥è¯†ä¸è¶³ä»¥å®Œæ•´å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å“ªéƒ¨åˆ†ä¿¡æ¯ä¸è¶³
3. å›ç­”è¦æ¸…æ™°ã€æœ‰æ¡ç†ï¼Œä½¿ç”¨ Markdown æ ¼å¼
4. å¦‚æœæ¶‰åŠä»£ç ï¼Œè¯·ä½¿ç”¨ä»£ç å—
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨å‚è€ƒæ¥æºï¼ˆç« èŠ‚ä¿¡æ¯ï¼‰

å‚è€ƒçŸ¥è¯†ï¼š
{evidence}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·å›ç­”ï¼š"""


class RAGService:
    """
    RAG ç»Ÿä¸€æœåŠ¡
    
    å®Œæ•´æµç¨‹ï¼š
    User Query â†’ Query Preprocess â†’ Multi-Retriever å¹¶è¡Œå¬å› 
    â†’ Candidate Merge & Dedup â†’ LLM Re-Ranking â†’ Top-K Evidence Selection 
    â†’ LLM Answer Generationï¼ˆGroundedï¼‰
    """
    
    def __init__(self):
        """åˆå§‹åŒ– RAG æœåŠ¡çš„æ‰€æœ‰ç»„ä»¶"""
        logger.info("ğŸš€ åˆå§‹åŒ– RAG æœåŠ¡...")
        
        # å…±äº«çš„ Qdrant å®¢æˆ·ç«¯
        qdrant_url = os.getenv("QDRANT_URL")
        qdrant_api_key = os.getenv("QDRANT_API_KEY")
        
        if not qdrant_url or not qdrant_api_key:
            raise ValueError("éœ€è¦é…ç½® QDRANT_URL å’Œ QDRANT_API_KEY")
        
        self.qdrant_client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            timeout=30
        )
        
        # å…±äº«çš„ Embedding å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()
        
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.dense_retriever = DenseRetriever(
            qdrant_client=self.qdrant_client,
            embedding_client=self.embedding_client
        )
        self.sparse_retriever = SparseRetriever()  # ä»ç£ç›˜åŠ è½½ç´¢å¼•
        self.section_retriever = SectionRetriever()
        self.splade_retriever = SpladeRetriever()   # SPLADE learned sparse retriever
        
        # è®¾ç½® section_retriever çš„ chunksï¼ˆå¤ç”¨ sparse çš„ï¼‰
        if self.sparse_retriever.chunks:
            self.section_retriever.set_chunks(self.sparse_retriever.chunks)
        
        # Multi-Retrieverï¼ˆå››è·¯æ£€ç´¢ï¼‰
        self.multi_retriever = MultiRetriever([
            self.dense_retriever,
            self.sparse_retriever,
            self.section_retriever,
            self.splade_retriever,
        ])
        
        # Re-Ranker
        self.reranker = LLMReranker()
        
        # ç­”æ¡ˆç”Ÿæˆ LLM
        api_key = os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        model_id = os.getenv("LLM_MODEL_ID", "deepseek-chat")
        
        if not api_key:
            api_key = os.getenv("DASHSCOPE_API_KEY")
            base_url = os.getenv("DASHSCOPE_BASE_URL")
            model_id = "qwen-plus"
        
        self.llm_client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_id = model_id
        
        self._initialized = True
        logger.info("âœ… RAG æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def answer(
        self,
        query: str,
        top_k_retrieve: int = 15,
        top_k_rerank: int = 5,
        use_llm_rerank: bool = False,
        use_query_rewrite: bool = True,
        use_rrf: bool = True,
        debug: bool = False,
    ) -> str:
        """
        å®Œæ•´ RAG æµç¨‹ï¼Œè¿”å›æœ€ç»ˆå›ç­”
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            top_k_retrieve: æ¯ä¸ª retriever å¬å›æ•°é‡
            top_k_rerank: re-ranking åä¿ç•™æ•°é‡
            use_llm_rerank: æ˜¯å¦ä½¿ç”¨ LLM è¿›è¡Œé‡æ’åºï¼ˆè€—æ—¶è¾ƒé•¿ä½†æ›´å‡†ç¡®ï¼‰
            use_query_rewrite: æ˜¯å¦å¯ç”¨ Query Rewrite
            use_rrf: æ˜¯å¦ä½¿ç”¨ RRF èåˆæ’åº
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ—¥å¿—
            
        Returns:
            LLM ç”Ÿæˆçš„æœ€ç»ˆå›ç­”
        """
        result = self.answer_with_evidence(
            query, top_k_retrieve, top_k_rerank, use_llm_rerank,
            use_query_rewrite=use_query_rewrite, use_rrf=use_rrf, debug=debug
        )
        return result.answer
    
    def answer_with_evidence(
        self, 
        query: str, 
        top_k_retrieve: int = 15, 
        top_k_rerank: int = 5,
        use_llm_rerank: bool = False,
        use_query_rewrite: bool = True,
        use_rrf: bool = True,
        debug: bool = False,
    ) -> RAGResult:
        """
        å®Œæ•´ RAG æµç¨‹ï¼Œè¿”å›å›ç­”å’Œè¯æ®
        
        æµç¨‹ï¼š
        1. Query Rewriteï¼ˆå¯é€‰ï¼‰
        2. å››è·¯å¹¶è¡Œæ£€ç´¢ï¼ˆDense + BM25 + Section + SPLADEï¼‰
        3. RRF èåˆæ’åºï¼ˆå¯é€‰ï¼Œå¦åˆ™ä½¿ç”¨æ—§çš„åˆå¹¶å»é‡ï¼‰
        4. Re-Rankingï¼ˆå¯é€‰ï¼‰
        5. LLM Answer Generation
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            top_k_retrieve: æ¯ä¸ª retriever å¬å›æ•°é‡
            top_k_rerank: re-ranking åä¿ç•™æ•°é‡
            use_llm_rerank: æ˜¯å¦ä½¿ç”¨ LLM è¿›è¡Œé‡æ’åº
            use_query_rewrite: æ˜¯å¦å¯ç”¨ Query Rewrite
            use_rrf: æ˜¯å¦ä½¿ç”¨ RRF èåˆæ’åº
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ—¥å¿—
            
        Returns:
            RAGResultï¼ˆå« answer å’Œ evidenceï¼‰
        """
        logger.info(f"ğŸ“ RAG æŸ¥è¯¢: {query}")
        timing = {}
        t_total_start = time.time()
        
        # Step 1: Query Rewrite
        t0 = time.time()
        if use_query_rewrite:
            processed_query = rewrite_query(query)
            logger.info(f"   Query Rewrite: '{query}' â†’ '{processed_query}'")
        else:
            processed_query = query.strip()
        timing["query_rewrite"] = round(time.time() - t0, 2)
        
        # Step 2: å››è·¯å¹¶è¡Œæ£€ç´¢ + èåˆ
        t0 = time.time()
        if use_rrf:
            candidates = self._retrieve_with_rrf(
                processed_query, top_k_retrieve, top_k_fused=top_k_retrieve, debug=debug
            )
        else:
            candidates = self.multi_retriever.retrieve(processed_query, top_k=top_k_retrieve)
        timing["retrieval_rrf"] = round(time.time() - t0, 2)
        
        logger.info(f"   æ£€ç´¢è¿”å› {len(candidates)} ä¸ªå€™é€‰")
        
        if not candidates:
            timing["total"] = round(time.time() - t_total_start, 2)
            return RAGResult(
                answer="æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•æ¢ä¸€ç§é—®æ³•æˆ–ç¡®è®¤çŸ¥è¯†åº“å·²å»ºç«‹ç´¢å¼•ã€‚",
                evidence=[],
                query=query,
                timing=timing
            )
        
        # Step 3: LLM Re-Ranking OR Simple Selection
        t0 = time.time()
        if use_llm_rerank:
            reranked = self.reranker.rerank(processed_query, candidates, top_k=top_k_rerank)
            logger.info(f"   LLM Re-Ranking åä¿ç•™ {len(reranked)} ä¸ª chunk")
        else:
            reranked = candidates[:top_k_rerank]
            logger.info(f"   Simple Selection (No LLM Rerank) ä¿ç•™ {len(reranked)} ä¸ª chunk")
        timing["reranking"] = round(time.time() - t0, 2)
        
        # Step 4: æ‹¼æ¥ Evidence
        evidence_text = self._format_evidence(reranked)
        
        # Step 5: LLM Answer Generationï¼ˆGroundedï¼‰
        t0 = time.time()
        answer = self._generate_answer(processed_query, evidence_text)
        timing["llm_generation"] = round(time.time() - t0, 2)
        
        timing["total"] = round(time.time() - t_total_start, 2)
        
        return RAGResult(
            answer=answer,
            evidence=reranked,
            query=query,
            timing=timing
        )
    
    def _retrieve_with_rrf(
        self,
        query: str,
        top_k_per_retriever: int = 15,
        top_k_fused: int = 15,
        debug: bool = False,
    ) -> List[RetrievedChunk]:
        """
        å››è·¯å¹¶è¡Œæ£€ç´¢ + RRF èåˆã€‚

        åˆ†åˆ«è°ƒç”¨ Denseã€BM25ã€Sectionã€SPLADE å››ä¸ªæ£€ç´¢å™¨ï¼Œ
        æ”¶é›†å„è‡ªçš„æœ‰åºç»“æœåˆ—è¡¨åä½¿ç”¨ RRF èåˆæ’åºã€‚

        Args:
            query: å¤„ç†åçš„æŸ¥è¯¢
            top_k_per_retriever: æ¯ä¸ªæ£€ç´¢å™¨è¿”å›çš„æ–‡æ¡£æ•°
            top_k_fused: èåˆåè¿”å›çš„æ–‡æ¡£æ€»æ•°
            debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯

        Returns:
            RRF èåˆåçš„ RetrievedChunk åˆ—è¡¨
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed

        retrievers = [
            ("dense", self.dense_retriever),
            ("bm25", self.sparse_retriever),
            ("section", self.section_retriever),
            ("splade", self.splade_retriever),
        ]

        results_per_retriever: List[List[RetrievedChunk]] = []

        with ThreadPoolExecutor(max_workers=len(retrievers)) as executor:
            futures = {}
            for name, retriever in retrievers:
                future = executor.submit(retriever.retrieve, query, top_k_per_retriever)
                futures[future] = name

            # æŒ‰æäº¤é¡ºåºæ”¶é›†ï¼ˆä¿æŒæ£€ç´¢å™¨é¡ºåºä¸€è‡´æ€§ï¼‰
            name_to_results: dict = {}
            for future in as_completed(futures):
                name = futures[future]
                try:
                    result = future.result()
                    name_to_results[name] = result
                    logger.info(f"   {name} æ£€ç´¢å™¨è¿”å› {len(result)} ä¸ªç»“æœ")
                except Exception as e:
                    logger.error(f"   {name} æ£€ç´¢å¤±è´¥: {e}")
                    name_to_results[name] = []

        # æŒ‰å›ºå®šé¡ºåºæ’åˆ—ï¼ˆDense, BM25, Section, SPLADEï¼‰
        for name, _ in retrievers:
            results_per_retriever.append(name_to_results.get(name, []))

        # è°ƒè¯•ï¼šæ‰“å°å„æ£€ç´¢å™¨ top3
        if debug:
            for (name, _), results in zip(retrievers, results_per_retriever):
                logger.info(f"[Debug] {name} Top-3:")
                for i, rc in enumerate(results[:3]):
                    logger.info(f"  {i+1}. {rc.chunk.chunk_id} | score={rc.score:.4f} | {rc.chunk.section}")

        # RRF èåˆ
        fused = reciprocal_rank_fusion(
            results_per_retriever,
            k=60,
            top_k=top_k_fused,
            debug=debug,
        )

        if debug:
            logger.info(f"[Debug] RRF èåˆå Top-5:")
            for i, rc in enumerate(fused[:5]):
                logger.info(
                    f"  {i+1}. {rc.chunk.chunk_id} | rrf={rc.score:.6f} | "
                    f"retrievers={rc.retriever_type} | {rc.chunk.section}"
                )

        return fused

    def _format_evidence(self, chunks: List[RetrievedChunk], max_chars_per_chunk: int = 0) -> str:
        """å°†æ£€ç´¢åˆ°çš„ chunk æ ¼å¼åŒ–ä¸º evidence æ–‡æœ¬
        
        Args:
            chunks: æ£€ç´¢ç»“æœ
            max_chars_per_chunk: æ¯ä¸ª chunk å†…å®¹çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œ0 è¡¨ç¤ºä¸æˆªæ–­
        """
        parts = []
        for i, rc in enumerate(chunks):
            content = rc.chunk.content
            if max_chars_per_chunk > 0 and len(content) > max_chars_per_chunk:
                content = content[:max_chars_per_chunk] + "..."
            part = (
                f"--- ç‰‡æ®µ {i+1} ---\n"
                f"æ¥æº: {rc.chunk.source}\n"
                f"ç« èŠ‚: {rc.chunk.chapter}"
            )
            if rc.chunk.section:
                part += f" > {rc.chunk.section}"
            part += f"\nå†…å®¹:\n{content}\n"
            parts.append(part)
        
        return "\n".join(parts)
    
    def _generate_answer(self, query: str, evidence: str) -> str:
        """è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›ç­”"""
        prompt = ANSWER_PROMPT_TEMPLATE.format(
            evidence=evidence,
            query=query
        )
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=2000
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"LLM å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}"
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥ RAG æœåŠ¡æ˜¯å¦å°±ç»ªï¼ˆç´¢å¼•æ˜¯å¦å·²å»ºç«‹ï¼‰"""
        try:
            collections = [c.name for c in self.qdrant_client.get_collections().collections]
            has_qdrant = "rag_knowledge_chunks" in collections
            has_bm25 = len(self.sparse_retriever.chunks) > 0
            return has_qdrant and has_bm25
        except Exception:
            return False

    # ------------------------------------------------------------------
    # è½»é‡ RAG ä¸Šä¸‹æ–‡æ£€ç´¢ï¼ˆä»…æ£€ç´¢ + èåˆ + é‡æ’ï¼Œä¸è°ƒ LLM ç”Ÿæˆç­”æ¡ˆï¼‰
    # ------------------------------------------------------------------
    def retrieve_context(
        self,
        query: str,
        top_k_retrieve: int = 10,
        top_k_rerank: int = 5,
        use_query_rewrite: bool = True,
        use_rrf: bool = True,
    ) -> tuple:
        """
        åªåšæ£€ç´¢ï¼Œè¿”å› (formatted_evidence, timing_dict)ã€‚

        ä¾› Learning / General Agent è·¯å¾„ä½¿ç”¨ï¼Œå°† RAG çŸ¥è¯†æ³¨å…¥
        LLM promptï¼Œè€Œéç”± RAG ç‹¬ç«‹ç”Ÿæˆç­”æ¡ˆã€‚

        Returns:
            (evidence_text: str, timing: dict)
        """
        timing: dict = {}
        t_total = time.time()

        # 1. Query Rewrite
        t0 = time.time()
        if use_query_rewrite:
            processed = rewrite_query(query)
        else:
            processed = query.strip()
        timing["query_rewrite"] = round(time.time() - t0, 2)

        # 2. å››è·¯æ£€ç´¢ + RRF
        t0 = time.time()
        if use_rrf:
            candidates = self._retrieve_with_rrf(processed, top_k_retrieve, top_k_fused=top_k_retrieve)
        else:
            candidates = self.multi_retriever.retrieve(processed, top_k=top_k_retrieve)
        timing["retrieval_rrf"] = round(time.time() - t0, 2)

        if not candidates:
            timing["total"] = round(time.time() - t_total, 2)
            return "", timing

        # 3. ç®€å•æˆªå–ï¼ˆä¸èµ° LLM rerankï¼Œä¿è¯é€Ÿåº¦ï¼‰
        top_chunks = candidates[:top_k_rerank]

        # 4. æ ¼å¼åŒ–ï¼ˆå–å®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­ï¼‰
        evidence = self._format_evidence(top_chunks)
        timing["total"] = round(time.time() - t_total, 2)
        return evidence, timing
